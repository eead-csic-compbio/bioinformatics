---
title: "Session 4: Benchmarks"
author:
- affiliation: Estaci√≥n Experimental de Aula Dei-CSIC, Zaragoza, Spain 
  name: Bruno Contreras Moreira, Najla Ksouri
output:
  html_document:
    fig_caption: yes
    highlight: zenburn
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: yes
    toc_depth: 3
  slidy_presentation:
    incremental: yes
  word_document: default
  html_notebook: 
    toc: yes
geometry: margin=1cm
fontsize: 11pt
bibliography: bib_myarticles_md.bib
---

```{r knitr setup, include=FALSE,  eval=TRUE, echo=FALSE, warning=FALSE}
library(knitr)
knitr::opts_chunk$set(eval=TRUE, cache=FALSE, message=FALSE, warning=FALSE, 
                      comment = "", results="markup")
```

<!-- 
https://www.kaggle.com/milesh1/receiver-operating-characteristic-roc-curve-in-r/log?select=winequality-red.csv
-->

# Introduction

In this session we will learn about benchmarks, which are experiments designed to measure the performance of algorithms, methodologies or statistical models. In particular, we will focus on classification problems. These are problems where data instances are assigned to a set of classes.  Note that methods that produce numeric scores can often be disguised as classifiers by adding rules that assign scores to classes. Neurons in neural nets use sigmoid functions for this, see [here](https://bioinfoperl.blogspot.com/2020/04/propagacion-en-una-red-neuronal.html) (in Spanish).

## Definitions

**Data instance:** individual observation within a data class. Example: a phenotype measurement on a plant, or from a single leaf.

**Training set:** Set of data used to train a model or algorithm. You can read about how deep learning machines are trained [here](https://bioinfoperl.blogspot.com/2020/05/entrenamiento-de-una-red-neuronal.html) (in Spanish).

**Validation set:** Fraction of the data used to checked the trained model during cross-validation.

**Test set:** Independent set of data used to measure the performance of a trained model or algorithm. If only one dataset is available then test and validations sets might be the same.

![Ways to partition a dataset for benchmarking. Dataset A only uses a training set and a test set. The test set would be used to test the trained model. For Dataset B, the validation set would be used to test the trained model, and the test set would evaluate the final model (@Km2020)](./pics/ML_dataset_training_validation_test_sets.png)

**Imbalanced dataset:** Occurs when the number of data instances in the training dataset in each class is not balanced. Severe imbalance happens when the distribution of instances is uneven by a large amount (e.g. 1:100 or more).

**Over-training:** Occurs when the performance on the training sets improve while getting worse on the validation sets. It is a form of over-fitting the training data. Learn more [here](https://bioinfoperl.blogspot.com/2020/05/sobreentrenamiento-de-una-red-neuronal.html) (in Spanish).

**Positive and negative cases:** When classifying data instances, positives are cases that truly belong to a class and negatives those that do not belong to it. The following figure, taken from [@Walber2014], summarizes how predictors or models handle instances and how the four possible outcomes (TP,FP,TN,FN) can be used to compute specificity and sensitivity:

![](./pics/Sensitivity_and_specificity.png)

**Sensitivity or recall:** 

$$ sens = \frac{TP}{TP+FN} $$

**Specificity or precision:**

$$ spec = \frac{TN}{TN+FP} $$

## A benchmark example


```{r gene_table}
annot.stats <- read.csv(file="test_data/Ensembl_stats.tsv", sep="\t", comment.char=";")
kable(annot.stats,format.args = list(big.mark=","))
```

## Exercise


# References and notes

